#!/usr/bin/env python3
import requests as rq
import re
from argparse import ArgumentParser as aAP
from urllib.parse import urljoin


parser = aAP(description="A simple script for downloading files from Sci-Hub",
             epilog="This script works by parsing the response "
             "sent back from the server. "
             "Since every Sci-Hub mirror may have different interfaces, "
             "and their layouts may change in the future, "
             "there is NO GUARANTEE that this script works on every mirror, "
             "or will continue to work on currently working mirrors"
             " in the future. ")
parser.add_argument("--version",
                    action='version',
                    version="%(prog)s v0.2 by KAC")
parser.add_argument("doi",
                    type=str,
                    nargs='*',
                    help="The doi string of the document")
parser.add_argument("--proxy",
                    type=str,
                    # default="socks5h://127.0.0.1:9150",
                    help="Requests-type proxy argument. "
                    "Used for both HTTP and HTTPS. "
                    "Use socks5h://127.0.0.1:9150 "
                    "for TOR browser socks5 proxy. "
                    # "Pass a empty string to disable proxy. "
                    "Default: "
                    "no proxy"
                    # "socks5h://127.0.0.1:9150"
                    )
parser.add_argument("--domain",
                    type=str,
                    default="https://sci-hub.se/",
                    help="Domain of scihub site to use. "
                    "Default: "
                    "https://sci-hub.se/")
parser.add_argument("--output",
                    type=str,
                    help="Save file with this name stem. "
                    "Default: "
                    "the remote file name")
parser.add_argument("--chunk",
                    type=int,
                    default=8192,
                    help="Size of each download chunk, in bytes. "
                    "Default: "
                    "8192")
parser.add_argument("--useragent",
                    type=str,
                    default="Mozilla/5.0 "
                    "(Windows NT 10.0; Win64; x64; rv:78.0) "
                    "Gecko/20100101 Firefox/78.0",
                    help="The user agent string used. "
                    "If this script fails to get results "
                    "but you can find the requested papers on the website, "
                    "try changing this. "
                    "Default: "
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) "
                    "Gecko/20100101 Firefox/78.0 ")
args = parser.parse_args()


def humanByteUnitString(s: int) -> str:
    if s >= 1024 ** 2:
        return f"{s / (1024 ** 2) :.2f} MB"
    elif s >= 1024:
        return f"{s / 1024 :.2f} KB"
    else:
        return f"{s} B"


proxyDict = {'http': args.proxy,
             'https': args.proxy} if (args.proxy is not None
                                      and args.proxy != "") else None
# check proxy
if proxyDict is not None:
    try:
        print("Testing proxy ...")
        rq.get("https://example.com/",
               proxies=proxyDict,
               headers={'User-Agent': args.useragent})
    except (rq.exceptions.InvalidURL, rq.exceptions.ProxyError):
        print("Proxy config is invalid")
        quit()
    except rq.ConnectionError:
        print("Failed connecting to requested proxy")
        quit()
    except Exception as e:
        print("Unknown exception: ")
        print(e)
        quit()
    else:
        print(f"Using proxy {args.proxy}")
else:
    print("No proxy configured")


if args.doi is None:
    args.doi = input("Enter the DOI string: ")
else:
    args.doi = " ".join(args.doi)
args.doi = re.sub("^(https?://)?(dx\\.|www\\.)?doi(\\.org/|:|/)\\s*",
                  '',
                  args.doi.strip())
# TODO better DOI checking
if ' ' in args.doi:
    print(f"Input DOI \"{args.doi}\" is invalid")
    quit()
print(f"Input DOI: {args.doi}")

# TODO check if different mirrors have different response
rePattern = re.compile("\"location\\.href=.?'(http.+?)\\?download=true'")

print("Querying " + urljoin(args.domain, args.doi))

firstResponse = rq.get(urljoin(args.domain, args.doi),
                       proxies=proxyDict,
                       headers={'User-Agent': args.useragent})

downloadLinkLst = []

# TODO better method of checking first return
if (not firstResponse.headers['Content-Type'].startswith('text/html')) \
        or len(firstResponse.text.strip('\n ')) == 0:
    print("No response. Maybe no search result?")
else:
    for line in firstResponse.text.splitlines():
        if 'download=true' in line:
            dlURL = rePattern.search(line).group(1)
            dlURL = dlURL.replace(r'\\', '\\').replace(r'\/', '/')
            print(f"Link found: {dlURL}")
            downloadLinkLst.append(dlURL)
    if len(downloadLinkLst) == 0:
        print("No download link detected in response. \n"
              "File may no be available on this mirror. \n"
              "Please check on the website manually. ")
        if len(firstResponse.text.strip('\n ')) < 50:
            print("Response text: ")
            for line in firstResponse.text.splitlines():
                print(line)
    else:
        if len(downloadLinkLst) > 1:
            print("Multiple download links found. ")
            print("They are: ")
            for lkn in downloadLinkLst:
                print(lkn)
            print("Will use the first link")
            downloadLinkLst = list(downloadLinkLst[0])
        else:
            docURL = downloadLinkLst[0]
            print(f"Downloading from {docURL} ...")
            dlFilename = docURL.rsplit('/')[-1]
            if args.output is not None:
                dlFilename = args.output + '.' + dlFilename.split('.')[-1]
            with rq.get(docURL,
                        proxies=proxyDict,
                        stream=True,
                        headers={'User-Agent': args.useragent}) as r:
                fileSize = r.headers.get('Content-Length', None)
                if fileSize is not None:
                    fileSize = int(fileSize)
                    print("File size: " + humanByteUnitString(fileSize))
                else:
                    print("File size unknown")
                downloadedSize = 0
                lastLineLen = 0
                dlMsg = None
                with open(dlFilename, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=args.chunk):
                        f.write(chunk)
                        downloadedSize += len(chunk)
                        if fileSize is not None:
                            dlMsg = "Downloaded " \
                                + f"{downloadedSize / fileSize * 100 :.2f}%"
                        else:
                            dlMsg = "Downloaded " \
                                + humanByteUnitString(downloadedSize)
                        print(dlMsg, end='')
                        lenDLMsg = len(dlMsg)
                        print(" " * (lastLineLen - lenDLMsg),
                              end='\r')
                        lastLineLen = lenDLMsg
            print("\nDownload done")
